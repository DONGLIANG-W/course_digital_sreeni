# importing data
pd.read_csv()
pd.read_excel()

# preview data
df.head()
df.tail()

# check the data type in the dataframe
df.dtypes

# convert the column to different data type
# convert the price series into float, original type could be string
df[['price']] = df[['price']].astype('float')

# quick check the stastical summay
df.describe()
df.describe(include='all')

# basic information of the dataset
df.info()

# access a column, each column called became a series 
df['column_name']

# dealing with missing values in dataframe
df.dropna() # axis=0 drops the entire row, axis=1 drops the entire column
# example, drop the rows which has nan in the price column, inplace=True
# replace the original df with the processed df
df.dropna(subset=['price'], axis=0, inplace=True)

# instead of drop the row or column, the missing value could be replaced
# by mean value
# example, calculate the mean of the column, then replace nan with mean
mean = df['normalized-losses'].mean()
df['normalized-losses'].replace(np.nan, mean)

# data formatting, 
# example convert 'miles per gallon' to 'L/100km' in car dataset
df['city-mpg'] = 235/df['city-mpg']
df.rename(columns={'city-mpg':'city-L/100km'}, inplace=True) 

# data normalization, columns may have very different scales, the 
# regression made based on the data may biased. Normalization may useful
# simple methods of normalizaing data, example, x_new = x_old/x_max
# alternative x_new = (x_old-mean)/stdev

# bin data into categories
# using np to create categories range, create a list which to be used as
# category names, use the pd.cut() to assign the bins
bins = np.linspace(min(df['price']), max(['price']), 4)
group_names = ['low','medium','high']
df['price-binned'] = pd.cut(df['price'], bins,labels=group_names, include_lowest=True)

# turning categorical variables into quantitative variables
# use the pandas.get_dummies() method convert categorical variables to 
# dummy variables (0 or 1)
pd.get_dummies(df['fuel'])

# exploratory data analysis
df.describe()
# descriptive statistics, summarize the categorical data is by using the
# value_count() method
drive_wheels_counts = df['drive-wheels'].value_counts().to_frame()

# scatter plot, here the course use plt.scatter() function
y = df['price']
x = df['engine-size']
plt.scatter(x,y)
plt.title("Scatter plot of engine size vs price")
plt.xlabel('engine size')
plt.ylable('price')

# boxplot, here the course use seaborn.boxplot() function
# pip install seaborn
import seaborn as sns
sns.boxplot(x='drive-wheels',y='price',data=df)

# groupBy function df.groupby()
# example group the data with two parameters, then get the mean value
df_grp = df.groupby(['drive-wheels', 'body-style'], as_index=False).mean()
# pd.pivot() function display one variable along the columns and the other 
# variable displayed along the rows - this method allows user to have a 
# clear view of the data grouped
df_pivot = df_grp.pivot(index='drive-wheels', columns='body-style')
# the correlation that pd.pivot() created could be presented ina heatmap
plt.pcolor(df_pivot, cmap='RdBu')
plt.colorbar()
plt.show()

# correlation - measure to what extent different variables are independent
# correlation between two features
sns.regplot(x='engine-size', y='price', data=df)
plt.ylim(0,)

# correlation - statistics
# Pearson correlation - measure the strength of the correlation between 
# two features
# two values given after computation, correction coefficient and P-value
# Correlation corefficent clost to 1 or -1 indicates the largest positive
# or negitive relationship, close to 0 indicates no relationship
# P-value < 0.001 indicates strong certainty in the result whereas larger
# P-value indicates weak certainty in the result
pearson_coef, p_value = stats.pearsonr(df['horsepower'], df['price'])

# association between two categorical variables: Chi-square
# different method used to measure the correlation for categorical variable
# compare to the continuous variable
# chi-square test for association
# the test is intended to test how likely it is that an observed distribution 
# is due to chance.
# the chi-square test a null hypothesis that variables are independent
# the chi-square does not tell you the type of relationship that exists 
# between both variables; but only that a relationship exists
# chi-square value is computed with formula 
# chi-square = sum(O_i-E_i)^2/E_i for i = 0 to i, whereas O_i is ith 
# observation, E_i is ith expectation
# after computation, check the lookup table, based on the degree of freedom
# and the chi-square value, get the probability value p. 
scipy.stats.chi2_contingency(cont_table, correction=True)





 




